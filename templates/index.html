<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Lite+Fog machine learning</title>
  <meta name="description" content=" Template for new projects.">

</head>

<body>
  <h1 style="color: #5e9ca0;">Lite+Fog AI & ML projects  </h1>
  <h2 style="color: #5e9ca0;"> (Development Server) </h2>

  <h2 style="color: #2e6c80;">Introduction</h2>
  <p><div class="line"><strong>Lite+Fog industries </strong>  is a vertical farming Berlin-based company, specialized in indoor farming of crops.</div></p>
  <p><div class="line"> Its AI unit, "Lite+Fog.ml", was established in order to equip its models prototypes with an optical modern AI- native data collection system for a quick and
  accurate plant analysis. </div></p>
  <p><div class="line"> Thus, its end-goal is to be described as the implementation of an embedded optical system, one which uses strong AI- algorithms for real-time plant phono-typing and understanding. </div></p>
  <p><div class="line"> As a bi-product of our guideline project, we got into developing a solid and robust Backend for the data collection and visualization, one which is cloud native in MS-Azure.  </div></p>
  <p><div class="line">In fact, our data pipelines start at the RT Embedded level - the plant level -  for the acquiring good optical data to be processed. Thereafter it is streamed into 3D reconstruction algorithms, preparing the data for feature extraction.</div></p>
  <p><div class="line">Using AI clustering algorithms and neural nets on these point clouds, we arrived at an assessment of the plant, and its growth prediction. </div></p>

  <h2 style="color: #2e6c80;">The experiment setup</h2>

  <p><div class="line"> We use 2 Raspberry Pi HQ cameras connected to a Raspberry Pi controller for taking videos of the plants from 2 angles. The vidoes are taken live and
  then streamed into an image database on Azure Cloud. </div></p>
  <p><div class="line"> The data is organized in a container and mapped as input to the a 3D-reconstruction software for generating point clouds data of the plants. </div></p>
  <p><div class="line"> The point clouds are stored in a cloud database and analyzed with Open3D for features extraction.</div></p>
  <div class="row">
    <div class="column">
        <p>{% load static %} <img src="{% static "expirementsetup1.jpg" %}" alt="home" width="400" height="500" /> </p>
    </div>
    <p><div class="line"> The white background is used for "fooling" the reconstruction software. Since usually it assumes the object is fixed and the cameras are moving, where for us it is the other way around. </div></p>
    <div class="column">
        <p>{% load static %} <img src="{% static "expirementsetup2.jpg" %}" alt="home" width="400" height="500" /> </p>
    </div>
  </div>
  <p>
    <div class="sketchfab-embed-wrapper">
          <iframe title="dense_inlier" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/8e27c4b4085a49efa306b243f23e8342/embed"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/dense-inlier-8e27c4b4085a49efa306b243f23e8342?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;"> dense_inlier </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div>
  </p>
</body>
</html>
