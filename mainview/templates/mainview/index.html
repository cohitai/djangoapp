{% extends "mainview/base.html" %}
{% load static %}

{% block title %}
    <div class="content">
        <h1 style="color: #2e6c80;">Lite+Fog AI Operations  </h1>
    </div>
{% endblock %}


{% block content %}
    <div class="content">
        <h3 style="color: #5e9ca0;">Introduction</h3>
        <p><div ><strong>Lite+Fog industries </strong>  is a Berlin-based vertical farming company, specialized in indoor farming of crops.</div></p>
        <p><div > Lite+Fog.ml is an AI research lab spinoff, specialized in machine learning research and computer vision development. Its guiding principle is to equip L+G prototypes with a modern optical AI native data collection systems for a nimble and
        accurate assessment of the plant phase. </div></p>
        <p><div >Thus, its end-goal may be described as the implementation of an embedded optical system, one which uses strong AI- algorithms for real-time plant phono-typing and understanding. </div></p>
        <p><div >As a bi-product of the guideline project, we got into developing a solid and robust Backend infrastructure for data collection, visualization and serving purposes, which is cloud native in MS-Azure.  </div></p>
        <p><div >In fact, our data pipelines starts at the RT Embedded level - plant level -  for acquiring sufficient optical data to be processed. Thereafter, it is streamed into complex 3D reconstruction algorithms, preparing the data for feature extraction.</div></p>
        <p><div >Using AI clustering algorithms and neural nets on these point clouds, we arrived at an assessment of the plant, and its growth prediction. </div></p>

        <h2 style="color: #2e6c80;">Interactive illustration from our lab</h2>
        <h4 style="color: #5e9ca0;">Experiment setup</h4>

        <p><div> The setup described below consists of 2 Raspberry Pi HQ webcam's, connected into a Raspberry Pi controller via Ethernet cables,
                 for shooting videos of the plants in 2 different angles. The vidoes are taken live and then streamed into an image database our Azure Cloud. </div></p>

        <p><img src="{% static "expirementsetup1.jpg" %}" alt="home" width="300" height="300" /> <img src="{% static "expirementsetup2.jpg" %}" alt="home" width="300" height="300" style="float:right" /> </p>

        <p><div> The white background is used for "fooling" the reconstruction software, since usually it is assumed that the object is fixed and the camera is in motion, whereas for us, it is the other way around. </div></p>
        <h4 style="color: #5e9ca0;">The raw data</h4>
        <p><div> The images are organized in containers and mapped as input/output to a containerized 3D-reconstruction software for generating point clouds data of the plants. </div></p>
        <p><div> The point clouds are stored in a cloud database and analyzed with Open3D for features extraction.</div></p>

        <!-- raw and masked data-->
        <div class="box">
            <div class="table">
                <div class="left">
                    <div class="nest"><img class="img" src="{% static "raw1.jpg" %}" /></div>
                    <div><img class="img" src="{% static "raw2.jpg" %}" /></div>
                </div>
                <div class="right">
                    <div class="nest"><img class="img" src="{% static "mask1.jpg" %}" /></div>
                    <div><img class="img" src="{% static "mask2.jpg" %}" /></div>
                </div>
            </div>
        </div>

        <h4 style="color: #5e9ca0;">3D reconstruction</h4>
        <p><div> Achieving a high quality point clouds is not trivial. Several camera parameters must be taken into consideration, such as the sharpness, brightness and the frame rate (fps).</div></p>
        <p><div> The rotation of the target object does not make it any simpler, what led us into involving complex masking algorithms for ruling out all background obstructions.</div></p>
        <p><div> The Sift matching algorithm for identifying similar pixels in different shots is visualized below. Note how similar pixels from different camera angles are being tracked, as well as those who comes from the objects' rotation.  </div></p>

        <!-- matching details pictures from COLMAP-->
        <div class="box">
            <div class="table">
                <div class="left"><img class="img" src="{% static "overlapping_imgs_1.jpg" %}" /></div>
                <div class="right">
                     <div class="nest"><img class="img" src="{% static "overlapping_imgs_2.jpg" %}" /></div>
                     <div><img class="img" src="{% static "sparse_with_cameras_positions.jpg" %}" /></div>
                </div>
             </div>
        </div>

        <h4 style="color: #5e9ca0;">Dense point clouds</h4>
        <p><div> At last we come at our destination, a 3D point cloud representation of the plant. We use these as the basis for our analysis. </div></p>
        <p><div> Below you can observe visualisations of the 3D basil plant, which we made through a dense reconstruction by SfM as explained above. </div></p>
        <p>
            <div class="sketchfab-embed-wrapper"> <iframe title="dense_inlier (6)" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/8cb32824aac84a6eb16b0bed91a04294/embed?ui_theme=dark"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/dense-inlier-6-8cb32824aac84a6eb16b0bed91a04294?utm_medium=embed&utm_campaign=share-popup&utm_content=8cb32824aac84a6eb16b0bed91a04294" target="_blank" style="font-weight: bold; color: #1CAAD9;"> dense_inlier (6) </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=8cb32824aac84a6eb16b0bed91a04294" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=8cb32824aac84a6eb16b0bed91a04294" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div>
        </p>
        <p>
            <div class="sketchfab-embed-wrapper"><iframe title="dense_inlier" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/8e27c4b4085a49efa306b243f23e8342/embed"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/dense-inlier-8e27c4b4085a49efa306b243f23e8342?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;"> dense_inlier </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=8e27c4b4085a49efa306b243f23e8342" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div>
        </p>


        <h4 style="color: #5e9ca0;"> Calibration with OpenCV</h4>

        <p>We describe below how we calibrate our cheap $50&nbsp;Raspberry Pi&#39;s HQ web camara using OpenCV module.</p>

        <p><img src="{% static "hqcameraraspberrypi.jpg" %}" class="center" alt="home" width="150" height="250" /></p>

        <p>Before we start, it will be prove worthy to keep in mind some of its technical specs. So we go ahead and look into the <a href="https://www.raspberrypi.com/documentation/accessories/camera.html">Raspberry Pi website</a>'s Hardware section, where we are able to collect the following notes for our experimental optical equipment. It has the <strong>Sony</strong> <strong>IMX477</strong>&nbsp;sensor with surface area of<strong> 6.287mm </strong>x<strong> 4.712 mm.</strong> The pixels size are the cubical <strong>1.55</strong> &micro;m x <strong>1.55</strong> &micro;m. The images produced are of&nbsp;<strong>4056 x 3040</strong> pixels.&nbsp;We chose the affordable  <strong>25$</strong> CS-mount lens with optical focal length: <strong>6mm</strong>. If our calibration is precise, we should be able to redeem this value at the end of the procedure.</p>

        <p>We printed 11 x 8 checkerboard pattern to be used as our target 3D real world calibration pattern.</p>

        <p>When everything is set, we start taking random shots of the checkerboard from general positions as possible, being careful not to fall into degenerate configurations, as explained in <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf"> Zhang's paper</a>. </p>

        <p> The OpenCV calibration code we use is extensively outlined in the <a href="https://docs.opencv.org/3.4/dc/dbb/tutorial_py_calibration.html"> official docs.<a></p>

        <video class="center" width="320" height="240" controls>
            <source src="{% static "rawframes.mp4" %}" type="video/mp4">
            <source src="movie.ogg" type="video/ogg">
                Your browser does not support the video tag.
        </video>

        <p>It consists of two tasks of pattern detection followed by the actual calibration solver. The first step is done with cv.findChessboardCorners and cv.cornerSubPix functions.  </p>
        <p>The calibration itself is done with cv.calibrateCamera.</p>
        <p>The second one is used to improve precision of the results, which we found influential for the process. </p>
        <p>As may be seen from the video below, the actual detected pattern is drawn with bright colors.. </p>

        <video class="center" width="320" height="240" controls>
            <source src="{% static "chessboarded.mp4" %}" type="video/mp4">
            <source src="movie.ogg" type="video/ogg">
                Your browser does not support the video tag.
        </video>

        <p>Having found enough 3D to 2D coordinates correspondence, we apply cv.calibrateCamera to it, for extracting the internal parameters of the camera.</p>
        <p>Explicitly, for this example, we received the following values: </p>
        <p>
            \begin{bmatrix} f_x & 0 & c_x\\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 3956.30286 & 0 & 2099.30134\\ 0 & 3949.07975 & 1568.10904 \\ 0 & 0 & 1 \end{bmatrix}
        </p>
        <p>with a re-projection error:  </p>
        <p class="center">
            error: = 0.21583277739214826,
        </p>
        <p>Which should be as close to zero as possible, for a good calibration. </p>
        <p>Using the undistorter option (cv.undistort), our corrected camera shots look as follows: </p>
        <video class="center" width="320" height="240" controls>
            <source src="{% static "undistorted.mp4" %}" type="video/mp4">
            <source src="movie.ogg" type="video/ogg">
                Your browser does not support the video tag.
        </video>

        <p>Last, we use the function cv.calibrationMatrixValues with the sensor parameters mentioned above, to see how close the acquired optical length with the given one (6mm), We got here: </p>
        <p class="center">
            (54.267371835571716, 42.0984458128734, 6.132464521011343, (3.2540205953986927, 2.430569016902181), 0.9981742764382634)
        </p>
        <p>The first two values are the field of view (FoV) angles, horizontally and vertically. The third value is the optical focal length, which seems rather close. The forth tuple value is the principal point. </p>

        <!--
        <h4 style="color: #5e9ca0;"> Volume and color Analysis</h4>
            <p> <img src="{% static "hqcameraraspberrypi.jpg" %}" alt="home" width="300" height="300" style="float:right" /> </p>
        -->
        <h4 style="color: #5e9ca0;">Training Nerf</h4>
        <p>Aside from the photogrammetry methods exemplified above, we were able to produce also points cloud using Neural Nets. The method is called <a href="https://github.com/bmild/nerf">"Nerf"<a>, Neural radiance fields, and is typically used for
           generating synthetic scenes from a sparse input of 2d images. The network comprises a crude network of 8 fully-connected layers, along a fine one which yields high quality reconstructions.  </p>
        <p>Together with its Hierarchical sampling method and positional encoding, we were able to run the algorithm successfully on our data, and to receive PSNR reconstruction score of up to 31, which is on-par to what stated in the paper.</p>
        <p>We give below demonstrations of our implementation. The training session, the mp4 output of the full network, and the point cloud which we were finally managed to acquire. </p>
        <p><img src="{% static "training_llffpipeline.gif" %}" class="center" alt="home" width="600" height="150" /></p>
        <p>On our highly capable machine, with Nvidia RTX 3090 GPU, it took us about 5 hours of training for the whole Network. </p>

        <video class="center" width="320" height="240" controls>
            <source src="{% static "test2_spiral_150000_rgb.mp4" %}" type="video/mp4">
            <source src="movie.ogg" type="video/ogg">
                Your browser does not support the video tag.
        </video>


        <p>and the "Nerfed" point cloud:</p>

        <p>
        <div class="sketchfab-embed-wrapper"> <iframe title="pcd_test2" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/712b9a8bdf6b4fbba5ee49cb59ef8cdf/embed"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/pcd-test2-712b9a8bdf6b4fbba5ee49cb59ef8cdf?utm_medium=embed&utm_campaign=share-popup&utm_content=712b9a8bdf6b4fbba5ee49cb59ef8cdf" target="_blank" style="font-weight: bold; color: #1CAAD9;"> pcd_test2 </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=712b9a8bdf6b4fbba5ee49cb59ef8cdf" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=712b9a8bdf6b4fbba5ee49cb59ef8cdf" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div>
        </p>

        <h4 style="color: #5e9ca0;">Plenoxol, Radiance Fields without Neural Networks. </h4>
        <p>Here we show our results using the latest code from <a href="https://github.com/sxyu/svox2">"Radiance Fields<a></a> without Neural Networks". This method is much faster than Nerf and produces roughly the same quality.</p>
        <p>Producing the following video took us 15 minutes, on our machine. </p>

        <video class="center" width="320" height="240" controls>
            <source src="{% static "plenoxels.mp4" %}" type="video/mp4">
            <source src="movie.ogg" type="video/ogg">
                Your browser does not support the video tag.
        </video>

        <p></p>The synthesized point cloud is given below, without the RGB colors. </p>
        <p><div class="sketchfab-embed-wrapper"> <iframe title="Plenoxels cloud (no RGB)" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/909d41af8c934081b4da1c4304c8452e/embed?ui_theme=dark"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/plenoxels-cloud-no-rgb-909d41af8c934081b4da1c4304c8452e?utm_medium=embed&utm_campaign=share-popup&utm_content=909d41af8c934081b4da1c4304c8452e" target="_blank" style="font-weight: bold; color: #1CAAD9;"> Plenoxels cloud (no RGB) </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=909d41af8c934081b4da1c4304c8452e" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=909d41af8c934081b4da1c4304c8452e" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div></p>



        <h4 style="color: #5e9ca0;">Poisson Mesh reconstruction from dense point clouds.  </h4>
        <div class="sketchfab-embed-wrapper"> <iframe title="Poisson Mesh" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/324ce9ec0cfe481ab46e2ad76b0565a7/embed?ui_theme=dark"> </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> <a href="https://sketchfab.com/3d-models/poisson-mesh-324ce9ec0cfe481ab46e2ad76b0565a7?utm_medium=embed&utm_campaign=share-popup&utm_content=324ce9ec0cfe481ab46e2ad76b0565a7" target="_blank" style="font-weight: bold; color: #1CAAD9;"> Poisson Mesh </a> by <a href="https://sketchfab.com/itai.cohen?utm_medium=embed&utm_campaign=share-popup&utm_content=324ce9ec0cfe481ab46e2ad76b0565a7" target="_blank" style="font-weight: bold; color: #1CAAD9;"> itai.cohen </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=324ce9ec0cfe481ab46e2ad76b0565a7" target="_blank" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a></p></div>
    </div>
{% endblock %}







